{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from openai import OpenAI\r\n",
    "import os\r\n",
    "import base64\r\n",
    "import requests\r\n",
    "import json\r\n",
    "import requests\r\n",
    "import pandas as pd\r\n",
    "import sys\r\n",
    "#Get the current working directory\r\n",
    "notebook_dir = os.getcwd()\r\n",
    "\r\n",
    "# Add the root directory to the Python path\r\n",
    "sys.path.append(os.path.abspath(os.path.join(notebook_dir, '..')))\r\n",
    "\r\n",
    "# Import the API key from config\r\n",
    "from config.config import API_KEY\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def gpt_authenticate():\r\n",
    "    try:\r\n",
    "        API_key = API_KEY\r\n",
    "        client = OpenAI(api_key=API_key)\r\n",
    "        \r\n",
    "        # Test the client with a simple API call\r\n",
    "        response = client.chat.completions.create(\r\n",
    "            model=\"gpt-3.5-turbo\",\r\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\r\n",
    "            max_tokens=5\r\n",
    "        )\r\n",
    "        print(\"Authentication successful!\")\r\n",
    "        return client\r\n",
    "        \r\n",
    "    except Exception as e:\r\n",
    "        print(f\"Authentication failed: {str(e)}\")\r\n",
    "        return None\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def check_image_url(url):\r\n",
    "    try:\r\n",
    "        # Send a GET request to the image URL\r\n",
    "        response = requests.get(url)\r\n",
    "        \r\n",
    "        # Check if the response status code is 200 (OK)\r\n",
    "        if response.status_code == 200:\r\n",
    "            # Get the Content-Type header from the response\r\n",
    "            content_type = response.headers.get('Content-Type', '')\r\n",
    "            \r\n",
    "            # Check if the Content-Type is an image format\r\n",
    "            if 'image' in content_type:\r\n",
    "                print(f\"Success: URL {url} is accessible and is of type {content_type}.\")\r\n",
    "                return False  # No error\r\n",
    "            else:\r\n",
    "                print(f\"Error: URL {url} is not an image. Content-Type: {content_type}\")\r\n",
    "                return True  # Error occurred\r\n",
    "        else:\r\n",
    "            print(f\"Error: URL {url} returned status code {response.status_code}.\")\r\n",
    "            return True  # Error occurred\r\n",
    "    \r\n",
    "    except requests.exceptions.RequestException as e:\r\n",
    "        print(f\"Error: Failed to access URL {url}. Exception: {e}\")\r\n",
    "        return True  # Error occurred"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "#%% using links as images\r\n",
    "def gpt_extract(client, url):\r\n",
    "    response = client.chat.completions.create(\r\n",
    "    model=\"gpt-4o\",\r\n",
    "    messages=[\r\n",
    "      {\r\n",
    "        \"role\": \"user\",\r\n",
    "        \"content\": [\r\n",
    "          {\"type\": \"text\", \"text\": '''Describe the image (Figure/Graphical abstract) from an article on comorbidity between COVID-19 and Neurodegeneration.   \r\n",
    "                                      1. Name potential mechanisms (pathophysiological processes) of Covid-19's impact on the brain depicted in the image. \r\n",
    "                                      2. Describe each process depicted in the image as semantic triples (subject–predicate–object).  \r\n",
    "                                      Example: \r\n",
    "                                      Pathophysiological Process: Astrocyte_Activation \r\n",
    "                                      Triples:\r\n",
    "                                      SARS-CoV-2_infection|triggers|astrocyte_activation\r\n",
    "                                      \r\n",
    "                                      Use ONLY the information shown in the image! Follow the structure precisely and don't write anything else! Replace spaces in names with _ sign, make sure that words \"Pathophysiology Process:\" and \"Triples:\" are presented, don't use bold font and margins. Each triple must contain ONLY THREE elements separated by a | sign, four and more are not allowed!'''},\r\n",
    "          {\r\n",
    "            \"type\": \"image_url\",\r\n",
    "            \"image_url\": {\r\n",
    "              \"url\": url,\r\n",
    "            },\r\n",
    "          },\r\n",
    "        ],\r\n",
    "      }\r\n",
    "    ],\r\n",
    "    max_tokens=900,\r\n",
    "  )\r\n",
    "    content = response.choices[0].message.content\r\n",
    "    return content"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "path = 'C:\\\\Users\\\\User\\\\Desktop\\\\Data\\\\SCAI\\\\'\r\n",
    "relevance_GPT = pd.read_excel(path+\"FINAL_RELEVANT_URLs.xlsx\")\r\n",
    "relevance_GPT.drop(['Unnamed: 0', 'Relevance_manual'], axis=1, inplace=True)\r\n",
    "relevance_GPT"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def triples_extraction_from_urls(path_URLs):\r\n",
    "\r\n",
    "    client = gpt_authenticate(API_key)\r\n",
    "    relevance_GPT = pd.read_excel(path_URLs)\r\n",
    "\r\n",
    "    # Initialize an empty list to store parsed data\r\n",
    "    parsed_data = []\r\n",
    "\r\n",
    "    # Loop through each row in the dataframe\r\n",
    "    for idx, row in (relevance_GPT.head(5)).iterrows():\r\n",
    "        try:\r\n",
    "            url = row[\"URL\"]  # Extract image URL\r\n",
    "\r\n",
    "            # Extract content from the image using GPT\r\n",
    "            content = gpt_extract(client, url)\r\n",
    "            print(url, content)\r\n",
    "\r\n",
    "            # Parse the text for mechanisms and triples\r\n",
    "            mechanisms = content.strip().split('Pathophysiology Process: ')\r\n",
    "\r\n",
    "            for mechanism_block in mechanisms[1:]:\r\n",
    "                lines = mechanism_block.strip().split('\\n')\r\n",
    "                mechanism_name = lines[0].strip()\r\n",
    "                triples = lines[2:]  # Skip the 'Triples:' line\r\n",
    "\r\n",
    "                for triple in triples:\r\n",
    "                    subject, action, obj = triple.strip().split('|')\r\n",
    "                    parsed_data.append([url, mechanism_name, subject, action, obj])\r\n",
    "\r\n",
    "        except Exception as e:\r\n",
    "            # Print the error and continue to the next row\r\n",
    "            print(f\"Error processing {url}: {e}\")\r\n",
    "            continue  # Skip to the next row in case of an error\r\n",
    "\r\n",
    "    # Create a new DataFrame to store parsed results\r\n",
    "    parsed_df = pd.DataFrame(parsed_data, columns=['URL', 'Pathophysiological Process', 'Subject', 'Predicate', 'Object'])\r\n",
    "\r\n",
    "    # Save dataframe\r\n",
    "    parsed_df.to_csv('Triples_Final.csv')\r\n",
    "    parsed_df.to_excel(r\"Triples_Final.xlsx\", index=False)\r\n",
    "    print('Triples_Final file is successfully saved as csv. and xlsx.')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# Save dataframe\r\n",
    "\r\n",
    "#parsed_df.to_csv('Triples_Final.csv')\r\n",
    "#parsed_df.to_excel(r\"Triples_Final.xlsx\", index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "\r\n",
    "# Load the data from both Excel files\r\n",
    "df1 = pd.read_excel('covid_neurodegeneration_triples.xlsx')\r\n",
    "df2 = pd.read_excel('covid_neurodegeneration_triples_2run.xlsx')\r\n",
    "\r\n",
    "# Step 1: Extract the relevant columns from both dataframes (Subject, Predicate, Object)\r\n",
    "triples_df1 = df1[['Subject', 'Predicate', 'Object']]\r\n",
    "triples_df2 = df2[['Subject', 'Predicate', 'Object']]\r\n",
    "\r\n",
    "# Step 2: Remove duplicate triples in both dataframes\r\n",
    "unique_triples_df1 = triples_df1.drop_duplicates()\r\n",
    "unique_triples_df2 = triples_df2.drop_duplicates()\r\n",
    "\r\n",
    "# Step 3: Merge the two dataframes to find common triples based on Subject, Predicate, and Object\r\n",
    "common_triples = pd.merge(unique_triples_df1, unique_triples_df2, on=['Subject', 'Predicate', 'Object'])\r\n",
    "\r\n",
    "# Step 4: Output the common triples\r\n",
    "print(\"Common triples:\")\r\n",
    "print(common_triples)\r\n",
    "\r\n",
    "# If you want to save the common triples to a file, uncomment the following line\r\n",
    "# common_triples.to_excel('common_triples.xlsx', index=False)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Separation into categories"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "parsed_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "source": [
    "print(len(set_t))\r\n",
    "set_t = set(parsed_df['Pathophysiological Process'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1086\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "set_t"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "source": [
    "categories_dict = {\r\n",
    "    'Viral Entry and Neuroinvasion': {\r\n",
    "        'ACE2': 3, 'TMPRSS2': 3, 'CD147': 3, 'receptor': 2, 'binding': 2, \r\n",
    "        'entry': 3, 'invasion': 3, 'spike_protein': 3, 'olfactory': 3, \r\n",
    "        'hematogenous': 3, 'blood-brain_barrier': 3, 'BBB': 3, 'neuroinvasion': 3, \r\n",
    "        'retrograde': 2, 'transport': 2, 'direct_infection': 3, 'direct_viral':2, 'neural': 2, \r\n",
    "        'neuron': 2, 'enteric_nervous_system': 3, 'route': 2, 'replication': 2, \r\n",
    "        'pathway': 1, 'transmission': 2, 'route':3\r\n",
    "    },\r\n",
    "    'Immune and Inflammatory Response': {\r\n",
    "        'immune': 2, 'cytokine': 3, 'storm': 3, 'IL': 2, 'interleukin': 2, \r\n",
    "        'TNFa': 3, 'T_cell': 3, 'B_cell': 3, 'inflammatory': 3, 'inflammation': 3, \r\n",
    "        'microglia': 2, 'astrocyte': 2, 'immune_response': 3, 'Th1': 2, \r\n",
    "        'Th17': 2, 'immunity': 2, 'macrophage': 2, 'leukocyte': 2, \r\n",
    "        'neutrophil': 2, 'antibody': 3, 'complement': 2, 'infection':2, 'signaling':3, 'glial_cell':3\r\n",
    "    },\r\n",
    "    'Cellular and Molecular Neurodegenerative Mechanisms': {\r\n",
    "        'amyloid': 3, 'tau': 3, 'synuclein': 3, 'prion': 3, 'aggregation': 2, \r\n",
    "        'misfolding': 2, 'oxidative_stress': 3, 'mitochondria': 3, \r\n",
    "        'neurodegeneration': 3, 'neurotoxicity': 3, 'neuronal_death': 3, \r\n",
    "        'apoptosis': 2, 'cell_death': 2, 'synapse': 2, 'excitotoxicity': 3, \r\n",
    "        'neurotransmitter': 2, 'synaptic_loss': 3, 'bioenergetic': 2, \r\n",
    "        'atrophy': 2, 'degradation': 2, 'dopamine':3, 'signaling':2, 'glia':2, 'grey_matter': 3, \r\n",
    "        'white_matter': 3, 'cortex': 3, 'posterior_fossa': 2, 'piriform_cortex': 2, 'hippocampus': 3, \r\n",
    "        'parahippocampal_gyrus': 3, 'amygdala': 2, 'basal_ganglia': 3, 'thalamus': 2, 'cerebellum': 2, \r\n",
    "        'prefrontal_cortex': 3, 'temporal_lobe': 3, 'frontal_lobe': 3, 'occipital_lobe': 2, 'parietal_lobe': 2, \r\n",
    "        'olfactory_bulb': 2, 'brainstem': 2, 'globus_pallidus': 2, 'substantia_nigra': 3\r\n",
    "    },\r\n",
    "    'Vascular-Related Neurodegenerative Effects': {\r\n",
    "        'endothelial': 3, 'vascular': 3, 'BBB_dysfunction': 3, 'coagulation': 3, \r\n",
    "        'fibrin': 2, 'thrombus': 3, 'microclot': 3, 'platelet': 2, \r\n",
    "        'thrombosis': 3, 'stroke': 3, 'ischemia': 3, 'hypoperfusion': 2, \r\n",
    "        'oxygenation': 2, 'microhemorrhage': 3\r\n",
    "    },\r\n",
    "    'Psychological and Neurological Symptoms': {\r\n",
    "        'depression': 3, 'depressive':3, 'anxiety': 3, 'cognitive': 3, 'memory': 3, \r\n",
    "        'brain_fog': 3, 'behavior': 2, 'mental': 2, 'psychological': 3, \r\n",
    "        'stress': 2, 'mood': 2, 'fatigue': 2, 'confusion': 2, \r\n",
    "        'neurological': 3, 'neurocognitive': 3, 'headache': 2, \r\n",
    "        'anosmia': 3, 'ageusia': 3, 'smell': 2, 'psychosis': 3, \r\n",
    "        'psychiatric': 3, 'neuromuscular': 2\r\n",
    "    },\r\n",
    "    'Systemic Cross-Organ Effects': {\r\n",
    "        'gut': 3, 'lung': 3, 'systemic': 3, 'cross-organ': 3, \r\n",
    "        'multi-organ': 3, 'metabolic': 2, 'blood': 2, 'microbiota': 3, \r\n",
    "        'dysbiosis': 3, 'liver': 2, 'gastrointestinal': 3, 'pulmonary': 2, \r\n",
    "        'kidney': 2, 'endocrine': 2, 'hormonal': 2, 'hormone':2, 'comorbidity': 2, \r\n",
    "        'microbiome': 3, 'organ': 2, 'HPA_axis': 3, 'alveol':2\r\n",
    "    }\r\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "source": [
    "from rapidfuzz import fuzz  # or from fuzzywuzzy import fuzz\r\n",
    "\r\n",
    "# Set a similarity threshold for fuzzy matching\r\n",
    "SIMILARITY_THRESHOLD = 80\r\n",
    "\r\n",
    "# Initialize the group dictionary with each category as a key and an empty list as the value\r\n",
    "group = {key: [] for key in categories_dict.keys()}\r\n",
    "group['Uncategorized'] = []\r\n",
    "\r\n",
    "# Loop over each item in the set to categorize it based on fuzzy matching\r\n",
    "for item in set_t:\r\n",
    "    # Convert the item to lowercase for case-insensitive matching\r\n",
    "    item_lower = item.lower()\r\n",
    "    \r\n",
    "    # Initialize category scores with 0 for each category; this will store the accumulated score\r\n",
    "    # for each category based on fuzzy matched keywords\r\n",
    "    category_scores = {key: 0 for key in categories_dict}\r\n",
    "    \r\n",
    "    # Calculate the score for each category by summing the weights of matching keywords\r\n",
    "    for category, keywords in categories_dict.items():\r\n",
    "        for keyword, weight in keywords.items():\r\n",
    "            # Convert the keyword to lowercase to match against the lowercase item text\r\n",
    "            # Use fuzzy matching to check if the keyword is similar to any part of the item text\r\n",
    "            similarity_score = fuzz.partial_ratio(keyword.lower(), item_lower)\r\n",
    "            if similarity_score >= SIMILARITY_THRESHOLD:\r\n",
    "                category_scores[category] += weight * (similarity_score / 100)  # Scale weight by similarity\r\n",
    "\r\n",
    "    # Find the maximum score among all categories\r\n",
    "    max_score = max(category_scores.values())\r\n",
    "    \r\n",
    "    #pat_process = item.split('@')[0]\r\n",
    "    pat_process = item\r\n",
    "\r\n",
    "    # Determine all categories that have the maximum score (if above zero)\r\n",
    "    if max_score > 0:\r\n",
    "        # Add item to each category that has the highest score\r\n",
    "        for category, score in category_scores.items():\r\n",
    "            if score == max_score:\r\n",
    "                group[category].append(pat_process)\r\n",
    "    else:\r\n",
    "        # If no keywords matched, add item to 'Uncategorized'\r\n",
    "        group['Uncategorized'].append(pat_process)\r\n",
    "\r\n",
    "# `group` now contains the categorized items, allowing items to be in multiple categories if they\r\n",
    "# have the same highest score across different categories\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "source": [
    "len(group['Uncategorized'])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "170"
      ]
     },
     "metadata": {},
     "execution_count": 210
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "group['Uncategorized']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "reverse_mapping_group = {value: key for key, values in group.items() for value in values}\r\n",
    "\r\n",
    "# Step 2: Map the values in 'existing_column' based on the reverse lookup\r\n",
    "parsed_df['Group'] = parsed_df['Pathophysiological Process'].map(reverse_mapping_group)\r\n",
    "parsed_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "source": [
    "#parsed_df.to_excel(r\"Triples_Final_Grouped.xlsx\", index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convert dois to pmids"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import requests\r\n",
    "import pandas as pd\r\n",
    "import time\r\n",
    "import re\r\n",
    "\r\n",
    "# Load the dataset\r\n",
    "file_path = \"../output/covid_neurodegeneration_triples.csv\"  # Update the path if needed\r\n",
    "df = pd.read_csv(file_path)\r\n",
    "\r\n",
    "# Function to clean DOI (remove \"https://doi.org/\" if present)\r\n",
    "def clean_doi(doi):\r\n",
    "    return re.sub(r\"https?://doi\\.org/\", \"\", str(doi).strip())\r\n",
    "\r\n",
    "# Function to convert DOI to PMID\r\n",
    "def fetch_pmid_from_doi(doi):\r\n",
    "    cleaned_doi = clean_doi(doi)\r\n",
    "    url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term={cleaned_doi}[doi]&retmode=json\"\r\n",
    "    try:\r\n",
    "        response = requests.get(url)\r\n",
    "        response.raise_for_status()\r\n",
    "        data = response.json()\r\n",
    "        pmid = data[\"esearchresult\"][\"idlist\"][0] if data[\"esearchresult\"][\"idlist\"] else None\r\n",
    "        return pmid\r\n",
    "    except Exception as e:\r\n",
    "        print(f\"Error fetching PMID for DOI {cleaned_doi}: {e}\")\r\n",
    "        return None\r\n",
    "\r\n",
    "# Apply DOI cleaning\r\n",
    "df[\"DOI_Cleaned\"] = df[\"DOI\"].apply(clean_doi)\r\n",
    "\r\n",
    "# Get unique cleaned DOIs\r\n",
    "unique_dois = df[\"DOI_Cleaned\"].dropna().unique()\r\n",
    "\r\n",
    "# Map DOI to PMID\r\n",
    "doi_to_pmid_mapping = {}\r\n",
    "for doi in unique_dois:\r\n",
    "    pmid = fetch_pmid_from_doi(doi)\r\n",
    "    doi_to_pmid_mapping[doi] = pmid\r\n",
    "    time.sleep(0.5)  # Respect NCBI API rate limits\r\n",
    "\r\n",
    "# Add mapped PMIDs to dataframe\r\n",
    "df[\"PMID\"] = df[\"DOI_Cleaned\"].map(doi_to_pmid_mapping)\r\n",
    "\r\n",
    "# Save updated file\r\n",
    "df.to_csv(\"covid_neurodegeneration_triples_with_pmids.csv\", index=False)\r\n",
    "\r\n",
    "# Display results\r\n",
    "print(df.head())\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fulltext triple extraction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Fulltext triple extraction on some test fulltexts\r\n",
    "import json\r\n",
    "import csv\r\n",
    "import os\r\n",
    "from openai import OpenAI\r\n",
    "\r\n",
    "def gpt_authenticate():\r\n",
    "    try:\r\n",
    "        API_key = API_KEY  # Replace with your actual API key\r\n",
    "        client = OpenAI(api_key=API_key)\r\n",
    "        # Test the client with a simple API call\r\n",
    "        response = client.chat.completions.create(\r\n",
    "            model=\"gpt-3.5-turbo\",\r\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\r\n",
    "            max_tokens=5\r\n",
    "        )\r\n",
    "        print(\"Authentication successful!\")\r\n",
    "        return client\r\n",
    "        \r\n",
    "    except Exception as e:\r\n",
    "        print(f\"Authentication failed: {str(e)}\")\r\n",
    "        return None\r\n",
    "\r\n",
    "def gpt_extract(client, text):\r\n",
    "    try:\r\n",
    "        response = client.chat.completions.create(\r\n",
    "            model=\"gpt-4\",  # or \"gpt-4-turbo-preview\"\r\n",
    "            messages=[\r\n",
    "                {\r\n",
    "                    \"role\": \"user\",\r\n",
    "                    \"content\": f'''Extract semantic triples (subject–predicate–object) from the paragraph below that describe the impact of COVID-19 on neurodegeneration:\r\n",
    "                    Paragraph: \"{text}\"\r\n",
    "                    \r\n",
    "                    Example: \r\n",
    "                    Pathophysiological Process: Astrocyte_Activation\r\n",
    "                    Triples:\r\n",
    "                    SARS-CoV-2_infection|triggers|astrocyte_activation\r\n",
    "                    \r\n",
    "                    Follow this format strictly:\r\n",
    "                    - Use ONLY the information from the paragraph.\r\n",
    "                    - Replace spaces in names with underscores (_).\r\n",
    "                    - Each triple must contain EXACTLY three elements separated by \"|\".\r\n",
    "                    - Ensure \"Pathophysiology Process:\" and \"Triples:\" appear in the response.\r\n",
    "                    '''\r\n",
    "                }\r\n",
    "            ],\r\n",
    "            max_tokens=900,\r\n",
    "            temperature=0.0  # Added for consistency\r\n",
    "        )\r\n",
    "        return response.choices[0].message.content\r\n",
    "    except Exception as e:\r\n",
    "        print(f\"Error in GPT extraction: {str(e)}\")\r\n",
    "        return \"\"\r\n",
    "\r\n",
    "def process_files(dataset_dir, csv_output_path):\r\n",
    "    client = gpt_authenticate()\r\n",
    "    if not client:\r\n",
    "        print(\"Failed to authenticate. Exiting...\")\r\n",
    "        return\r\n",
    "\r\n",
    "    extracted_triples = []\r\n",
    "    processed_count = 0\r\n",
    "\r\n",
    "    try:\r\n",
    "        # Create output directory if it doesn't exist\r\n",
    "        os.makedirs(os.path.dirname(csv_output_path), exist_ok=True)\r\n",
    "\r\n",
    "        for filename in os.listdir(dataset_dir):\r\n",
    "            if filename.endswith(\".json\"):\r\n",
    "                pmid = filename.split('.')[0]  # Remove .json extension\r\n",
    "                print(f\"\\nProcessing file {processed_count + 1}: {filename}\")\r\n",
    "                file_path = os.path.join(dataset_dir, filename)\r\n",
    "                \r\n",
    "                try:\r\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as file:\r\n",
    "                        data = json.load(file)\r\n",
    "                        \r\n",
    "                        if \"paragraphs\" in data:\r\n",
    "                            for paragraph_idx, paragraph in enumerate(data[\"paragraphs\"]):\r\n",
    "                                if not paragraph or not paragraph.strip():\r\n",
    "                                    continue\r\n",
    "                                    \r\n",
    "                                print(f\"Processing paragraph {paragraph_idx + 1}\")\r\n",
    "                                extracted_text = gpt_extract(client, paragraph)\r\n",
    "                                \r\n",
    "                                if extracted_text:\r\n",
    "                                    lines = extracted_text.split(\"\\n\")\r\n",
    "                                    for line in lines:\r\n",
    "                                        if \"|\" in line:\r\n",
    "                                            parts = line.strip().split(\"|\")\r\n",
    "                                            if len(parts) == 3:\r\n",
    "                                                subject, relation, obj = parts\r\n",
    "                                                triple = [\r\n",
    "                                                    pmid,\r\n",
    "                                                    subject.strip(),\r\n",
    "                                                    relation.strip(),\r\n",
    "                                                    obj.strip()\r\n",
    "                                                ]\r\n",
    "                                                extracted_triples.append(triple)\r\n",
    "                                                print(f\"Extracted triple: {' | '.join(triple)}\")\r\n",
    "                \r\n",
    "                except Exception as e:\r\n",
    "                    print(f\"Error processing file {filename}: {str(e)}\")\r\n",
    "                    continue\r\n",
    "                \r\n",
    "                processed_count += 1\r\n",
    "                \r\n",
    "                # Save progress periodically (every 5 files)\r\n",
    "                if processed_count % 5 == 0:\r\n",
    "                    save_to_csv(extracted_triples, csv_output_path)\r\n",
    "                    print(f\"\\nProgress saved: {processed_count} files processed\")\r\n",
    "\r\n",
    "        # Final save\r\n",
    "        save_to_csv(extracted_triples, csv_output_path)\r\n",
    "        print(f\"\\nProcessing complete! Total files processed: {processed_count}\")\r\n",
    "        print(f\"Total triples extracted: {len(extracted_triples)}\")\r\n",
    "        print(f\"Results saved to: {csv_output_path}\")\r\n",
    "\r\n",
    "    except Exception as e:\r\n",
    "        print(f\"Fatal error in processing: {str(e)}\")\r\n",
    "        # Save what we have in case of fatal error\r\n",
    "        save_to_csv(extracted_triples, csv_output_path)\r\n",
    "\r\n",
    "def save_to_csv(triples, csv_path):\r\n",
    "    try:\r\n",
    "        with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\r\n",
    "            csv_writer = csv.writer(csvfile)\r\n",
    "            csv_writer.writerow([\"PMID\", \"Subject\", \"Relation\", \"Object\"])\r\n",
    "            csv_writer.writerows(triples)\r\n",
    "    except Exception as e:\r\n",
    "        print(f\"Error saving to CSV: {str(e)}\")\r\n",
    "\r\n",
    "if __name__ == \"__main__\":\r\n",
    "    # Define file paths\r\n",
    "    dataset_dir = \"../data/test_fulltext/chunked_files\"\r\n",
    "    outputsave = \"../data/test_fulltext\"\r\n",
    "    csv_output_path = os.path.join(outputsave, \"extracted_triples.csv\")\r\n",
    "    \r\n",
    "    # Run the extraction process\r\n",
    "    process_files(dataset_dir, csv_output_path)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.12.0 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "interpreter": {
   "hash": "d6ca1284c7bd7ae5870c8ab738ede617c834b0c6d27e733d7d2a522b2e1e0f98"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}