{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42597f44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78ec6231-61d6-4bb0-8aa5-183e9c3ee5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(set1, set2):\n",
    "    \"\"\"\n",
    "    Calculate Jaccard similarity between two sets.\n",
    "    \"\"\"\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "def image_specific_jaccard_similarity(df_1, df_2, image_url):\n",
    "    \"\"\"\n",
    "    Compute Jaccard similarity for a specific image between manual and GPT triples.\n",
    "    \"\"\"\n",
    "    # Filter dataframes by the specific image URL\n",
    "    df_1 = df_1[df_1['URL'] == image_url]\n",
    "    df_2 = df_2[df_2['URL'] == image_url]\n",
    "    \n",
    "    # Combine the Subject, Predicate, Object columns into a single triple string\n",
    "    df_1 = df_1.copy()\n",
    "    df_1.loc[:, 'Triple'] = df_1['Subject'] + \" \" + df_1['Predicate'] + \" \" + df_1['Object']\n",
    "    \n",
    "    df_2 = df_2.copy()\n",
    "    df_2.loc[:, 'Triple'] = df_2['Subject'] + \" \" + df_2['Predicate'] + \" \" + df_2['Object']\n",
    "\n",
    "    # Initialize variables to store total similarity\n",
    "    total_similarity = 0\n",
    "    for triple1 in df_1['Triple']:\n",
    "        # Tokenize the first triple\n",
    "        set1 = set(triple1.lower().split())\n",
    "        \n",
    "        # Compute similarity with all triples in GPT data\n",
    "        similarities = []\n",
    "\n",
    "        for triple2 in df_2['Triple']:\n",
    "            set2 = set(triple2.lower().split())\n",
    "            similarity = jaccard_similarity(set1, set2)\n",
    "            similarities.append(similarity)\n",
    "        \n",
    "        # Use the highest similarity for this triple\n",
    "        total_similarity += max(similarities) if similarities else 0\n",
    "    \n",
    "    # Normalize by the number of triples in the manual data\n",
    "    overall_similarity = total_similarity / len(df_1) if len(df_1) > 0 else 0\n",
    "    return overall_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b1a3cf6-6f26-4680-9456-a668fd64ada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data for the two runs\n",
    "path = r\"C:\\Users\\User\\Desktop\\Data\\SCAI\\Image_analysis_AI\\Supplementary_material_Table_1.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4ec38b4-d9b2-4afe-b526-16fdb0c5cce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs12640-020-00219-8/MediaObjects/12640_2020_219_Fig1_HTML.png\n",
      "  Jaccard Similarity (Manual vs. Prompt_1): 0.13\n",
      "  Jaccard Similarity (Manual vs. Prompt_2): 0.14\n",
      "  Jaccard Similarity (Manual vs. Prompt_3): 0.15\n",
      "Image: https://www.eurekaselect.com/images/graphical-abstract/cn/20/10/015.jpg\n",
      "  Jaccard Similarity (Manual vs. Prompt_1): 0.29\n",
      "  Jaccard Similarity (Manual vs. Prompt_2): 0.20\n",
      "  Jaccard Similarity (Manual vs. Prompt_3): 0.26\n",
      "Image: https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs11033-024-09279-x/MediaObjects/11033_2024_9279_Fig2_HTML.png\n",
      "  Jaccard Similarity (Manual vs. Prompt_1): 0.63\n",
      "  Jaccard Similarity (Manual vs. Prompt_2): 0.20\n",
      "  Jaccard Similarity (Manual vs. Prompt_3): 0.56\n",
      "\n",
      "Mean Jaccard Similarity (Manual vs. Prompt_1): 0.35\n",
      "Mean Jaccard Similarity (Manual vs. Prompt_2): 0.18\n",
      "Mean Jaccard Similarity (Manual vs. Prompt_3): 0.32\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "manual_data = pd.read_excel(path, sheet_name='Manual')\n",
    "prompt_1_data = pd.read_excel(path, sheet_name='Prompt_1')\n",
    "prompt_2_data = pd.read_excel(path, sheet_name='Prompt_2')\n",
    "prompt_3_data = pd.read_excel(path, sheet_name='Prompt_3')\n",
    "\n",
    "# Define image URLs to compare\n",
    "image_urls = manual_data['URL'].unique()\n",
    "\n",
    "# Initialize lists to store Jaccard similarity scores\n",
    "similarities_prompt_1 = []\n",
    "similarities_prompt_2 = []\n",
    "similarities_prompt_3 = []\n",
    "\n",
    "# Compare similarities for each image across different GPT prompt outputs\n",
    "for image_url in image_urls:\n",
    "    similarity_prompt_1 = image_specific_jaccard_similarity(manual_data, prompt_1_data, image_url)\n",
    "    similarity_prompt_2 = image_specific_jaccard_similarity(manual_data, prompt_2_data, image_url)\n",
    "    similarity_prompt_3 = image_specific_jaccard_similarity(manual_data, prompt_3_data, image_url)\n",
    "    \n",
    "    # Append scores to respective lists\n",
    "    similarities_prompt_1.append(similarity_prompt_1)\n",
    "    similarities_prompt_2.append(similarity_prompt_2)\n",
    "    similarities_prompt_3.append(similarity_prompt_3)\n",
    "    \n",
    "    # Print scores for each image\n",
    "    print(f\"Image: {image_url}\")\n",
    "    print(f\"  Jaccard Similarity (Manual vs. Prompt_1): {similarity_prompt_1:.2f}\")\n",
    "    print(f\"  Jaccard Similarity (Manual vs. Prompt_2): {similarity_prompt_2:.2f}\")\n",
    "    print(f\"  Jaccard Similarity (Manual vs. Prompt_3): {similarity_prompt_3:.2f}\")\n",
    "\n",
    "# Calculate mean similarities for each comparison\n",
    "mean_similarity_prompt_1 = sum(similarities_prompt_1) / len(similarities_prompt_1) if similarities_prompt_1 else 0\n",
    "mean_similarity_prompt_2 = sum(similarities_prompt_2) / len(similarities_prompt_2) if similarities_prompt_2 else 0\n",
    "mean_similarity_prompt_3 = sum(similarities_prompt_3) / len(similarities_prompt_3) if similarities_prompt_3 else 0\n",
    "\n",
    "# Print the mean scores\n",
    "print(f\"\\nMean Jaccard Similarity (Manual vs. Prompt_1): {mean_similarity_prompt_1:.2f}\")\n",
    "print(f\"Mean Jaccard Similarity (Manual vs. Prompt_2): {mean_similarity_prompt_2:.2f}\")\n",
    "print(f\"Mean Jaccard Similarity (Manual vs. Prompt_3): {mean_similarity_prompt_3:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab8a1dee-5bdc-420f-bef2-d7f01742c174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs12640-020-00219-8/MediaObjects/12640_2020_219_Fig1_HTML.png\n",
      "  Jaccard Similarity (Prompt_1 first run vs. Prompt_1 second run): 0.39\n",
      "  Jaccard Similarity (Prompt_1 first run vs. Prompt_1 third run): 0.26\n",
      "  Jaccard Similarity (Prompt_1 first run (temperature = 1; top_p = 1) vs. Prompt_1 (temperature = 0.5; top_p = 0.5)): 0.19\n",
      "  Jaccard Similarity (Prompt_1 first run (temperature = 1; top_p = 1) vs. Prompt_1 (temperature = 0.0; top_p = 0.0)): 0.31\n",
      "Image: https://media.springernature.com/lw1200/springer-static/image/art%3A10.1007%2Fs11033-024-09279-x/MediaObjects/11033_2024_9279_Fig2_HTML.png\n",
      "  Jaccard Similarity (Prompt_1 first run vs. Prompt_1 second run): 0.42\n",
      "  Jaccard Similarity (Prompt_1 first run vs. Prompt_1 third run): 0.51\n",
      "  Jaccard Similarity (Prompt_1 first run (temperature = 1; top_p = 1) vs. Prompt_1 (temperature = 0.5; top_p = 0.5)): 0.66\n",
      "  Jaccard Similarity (Prompt_1 first run (temperature = 1; top_p = 1) vs. Prompt_1 (temperature = 0.0; top_p = 0.0)): 0.83\n",
      "Image: https://www.eurekaselect.com/images/graphical-abstract/cn/20/10/015.jpg\n",
      "  Jaccard Similarity (Prompt_1 first run vs. Prompt_1 second run): 0.40\n",
      "  Jaccard Similarity (Prompt_1 first run vs. Prompt_1 third run): 0.33\n",
      "  Jaccard Similarity (Prompt_1 first run (temperature = 1; top_p = 1) vs. Prompt_1 (temperature = 0.5; top_p = 0.5)): 0.33\n",
      "  Jaccard Similarity (Prompt_1 first run (temperature = 1; top_p = 1) vs. Prompt_1 (temperature = 0.0; top_p = 0.0)): 0.33\n",
      "Image: https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs10194-022-01450-8/MediaObjects/10194_2022_1450_Fig1_HTML.png\n",
      "  Jaccard Similarity (Prompt_1 first run vs. Prompt_1 second run): 0.35\n",
      "  Jaccard Similarity (Prompt_1 first run vs. Prompt_1 third run): 0.27\n",
      "  Jaccard Similarity (Prompt_1 first run (temperature = 1; top_p = 1) vs. Prompt_1 (temperature = 0.5; top_p = 0.5)): 0.68\n",
      "  Jaccard Similarity (Prompt_1 first run (temperature = 1; top_p = 1) vs. Prompt_1 (temperature = 0.0; top_p = 0.0)): 0.47\n",
      "Image: https://qeios-uploads.s3.eu-west-1.amazonaws.com/editor/f7r5YwRp0BQmTWp3AnGmXRx8JTRQYTgq2XkQLQl9.png\n",
      "  Jaccard Similarity (Prompt_1 first run vs. Prompt_1 second run): 0.45\n",
      "  Jaccard Similarity (Prompt_1 first run vs. Prompt_1 third run): 0.31\n",
      "  Jaccard Similarity (Prompt_1 first run (temperature = 1; top_p = 1) vs. Prompt_1 (temperature = 0.5; top_p = 0.5)): 0.38\n",
      "  Jaccard Similarity (Prompt_1 first run (temperature = 1; top_p = 1) vs. Prompt_1 (temperature = 0.0; top_p = 0.0)): 0.45\n",
      "\n",
      "Mean Jaccard Similarity (Prompt_1 first run vs. Prompt_1 second run): 0.40\n",
      "Mean Jaccard Similarity (Prompt_1 first run vs. Prompt_1 third run): 0.34\n",
      "Mean Jaccard Similarity (Prompt_1 first run (temperature = 1; top_p = 1) vs. Prompt_1 (temperature = 0.5; top_p = 0.5)): 0.45\n",
      "Mean Jaccard Similarity (Prompt_1 first run (temperature = 1; top_p = 1) vs. Prompt_1 (temperature = 0.0; top_p = 0.0)): 0.48\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data_run1 = pd.read_excel(path, sheet_name='Prompt_1')\n",
    "data_run2 = pd.read_excel(path, sheet_name='Prompt_1_1')\n",
    "data_run3 = pd.read_excel(path, sheet_name='Prompt_1_2')\n",
    "data_run_0_5 = pd.read_excel(path, sheet_name='Prompt_1_Parameters_0.5')\n",
    "data_run_0_0 = pd.read_excel(path, sheet_name='Prompt_1_Parameters_0.0')\n",
    "\n",
    "# Define image URLs to compare\n",
    "image_urls_0 = data_run1['URL'].unique()\n",
    "\n",
    "# Initialize lists to store scores for each comparison\n",
    "similarity_first_second = []\n",
    "similarity_first_third = []\n",
    "similarity_first_05 = []\n",
    "similarity_first_00 = []\n",
    "\n",
    "# Loop through each image URL\n",
    "for image_url in image_urls_0:\n",
    "    score_first_second = image_specific_jaccard_similarity(data_run1, data_run2, image_url)\n",
    "    score_first_third = image_specific_jaccard_similarity(data_run1, data_run3, image_url)\n",
    "    score_first_05 = image_specific_jaccard_similarity(data_run1, data_run_0_5, image_url)\n",
    "    score_first_00 = image_specific_jaccard_similarity(data_run1, data_run_0_0, image_url)\n",
    "\n",
    "    # Append scores to respective lists\n",
    "    similarity_first_second.append(score_first_second)\n",
    "    similarity_first_third.append(score_first_third)\n",
    "    similarity_first_05.append(score_first_05)\n",
    "    similarity_first_00.append(score_first_00)\n",
    "\n",
    "    print(f\"Image: {image_url}\")\n",
    "    print(f\"  Jaccard Similarity (Prompt_1 first run vs. Prompt_1 second run): {score_first_second:.2f}\")\n",
    "    print(f\"  Jaccard Similarity (Prompt_1 first run vs. Prompt_1 third run): {score_first_third:.2f}\")\n",
    "    print(f\"  Jaccard Similarity (Prompt_1 first run (temperature = 1; top_p = 1) vs. Prompt_1 (temperature = 0.5; top_p = 0.5)): {score_first_05:.2f}\")\n",
    "    print(f\"  Jaccard Similarity (Prompt_1 first run (temperature = 1; top_p = 1) vs. Prompt_1 (temperature = 0.0; top_p = 0.0)): {score_first_00:.2f}\")\n",
    "\n",
    "# Calculate mean similarity scores\n",
    "mean_first_second = sum(similarity_first_second) / len(similarity_first_second) if similarity_first_second else 0\n",
    "mean_first_third = sum(similarity_first_third) / len(similarity_first_third) if similarity_first_third else 0\n",
    "mean_first_05 = sum(similarity_first_05) / len(similarity_first_05) if similarity_first_05 else 0\n",
    "mean_first_00 = sum(similarity_first_00) / len(similarity_first_00) if similarity_first_00 else 0\n",
    "\n",
    "# Print the mean scores\n",
    "print(f\"\\nMean Jaccard Similarity (Prompt_1 first run vs. Prompt_1 second run): {mean_first_second:.2f}\")\n",
    "print(f\"Mean Jaccard Similarity (Prompt_1 first run vs. Prompt_1 third run): {mean_first_third:.2f}\")\n",
    "print(f\"Mean Jaccard Similarity (Prompt_1 first run (temperature = 1; top_p = 1) vs. Prompt_1 (temperature = 0.5; top_p = 0.5)): {mean_first_05:.2f}\")\n",
    "print(f\"Mean Jaccard Similarity (Prompt_1 first run (temperature = 1; top_p = 1) vs. Prompt_1 (temperature = 0.0; top_p = 0.0)): {mean_first_00:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c312398e-0975-4d2a-bcf1-3cb2284328b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193f8f97-910b-46f7-b95e-0baeb6258d39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
